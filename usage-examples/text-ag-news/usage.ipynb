{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637e8e9a",
   "metadata": {},
   "source": [
    "# AG-News\n",
    "\n",
    "Topic classification task.\n",
    "\n",
    "Demonstrates:\n",
    "* Implementing a DataReader\n",
    "* Embedding textual data in preparation for feeding to CCP with byte-pair embeddings\n",
    "* Learning soft pseudo-labels as q-vectors via Contrastive Credibility Propagation\n",
    "* Classifying test examples\n",
    "\n",
    "Evaluates:\n",
    "* How well the labeler recovers q-vectors\n",
    "* How well the classifier performs classification\n",
    "    * (a) supervised learning with all 120k training examples (best case),\n",
    "    * (b) supervised learning with missing labels (worst case),\n",
    "    * (c) supervised learning with a subset of the training labels forgotten and then recovered as soft labels with CCP (CCP case)\n",
    "\n",
    "This implementation automatically runs on a single GPU device named `cuda:0` if CUDA is available. If unavailable, it defaults to using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b05183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s %(levelname)s %(name)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a370e2",
   "metadata": {},
   "source": [
    "## Dataset, embedding that dataset, and loading batches of that dataset\n",
    "\n",
    "`AgNewsDataReader` is a `DataReader`. If you want to transform the data, you should pass a Callable as the `transform` parameter. If you want the raw strings, you should pass `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e4e1e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ccp.encoders.byte_pair_embed import BytePairEmbed\n",
    "from agnews_ccp_datareader import AgNewsDataReader\n",
    "\n",
    "VOCABULARY_SIZE = 50_000\n",
    "EMBEDDING_DIMENSIONALITY = 100\n",
    "FORCED_LENGTH = 512\n",
    "\n",
    "PROJECTION_HIDDEN_DIM = 64\n",
    "PROJECTION_OUTPUT_DIM = 32 # representation of PROJECTION_OUTPUT_DIM size is used to calculate the soft supervised contrastive loss (L_SSC) for CCP pseudo-labeling\n",
    "\n",
    "# CCP becomes particularly with few examples (e.g., MISSING_DATA_RATE around 0.99 or 0.999)\n",
    "MISSING_DATA_RATE = 0.2\n",
    "# sample_indices_to_use is populated so this code runs quickly on a CPU; it should be set to `None` to learn \n",
    "SAMPLE_INDICES = [0, 1, 2, 5000, 5001, 5002, 10000, 10001, 15000, 15001, 20000, 20001, 25000, 25001, 30000, 30001, 100000, 100001, 110000, 110001]\n",
    "\n",
    "\n",
    "embed = BytePairEmbed(output_size=FORCED_LENGTH,\n",
    "                      vocab_size=VOCABULARY_SIZE, \n",
    "                      embedding_dimensionality=EMBEDDING_DIMENSIONALITY)\n",
    "\n",
    "ag_data = AgNewsDataReader(split=\"train\", \n",
    "                           rate_to_keep=1-MISSING_DATA_RATE,\n",
    "                           transform=embed,\n",
    "                           sample_indices_to_use = SAMPLE_INDICES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348e5c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training CCP\n",
    "\n",
    "CCP consists of two models, which share some substructure:\n",
    "* The softlabeler ContrastiveCredibilityLabeller, which produces q-vectors for each sample using an encoder network f_b(x) and a projection head f_z(f_b(x)).\n",
    "* The classifier ContrastiveCredibilityClassifier, which is a classification model for the target space. The model uses the same encoder network f_b(x) and a separate projection head f_g(f_b(x)).\n",
    "\n",
    "As a computational performance optimization, you can optionally prewarm the network state, and reuse that state to reinitialize between iterations of CCP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07bda98",
   "metadata": {},
   "source": [
    "### Configure the soft labeling model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e42eae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from ccp.softlabeler import ContrastiveCredibilityLabeller\n",
    "from ccp.softlabeler.transforms.generic_transforms import Identity, GaussianNoise\n",
    "from ccp.softlabeler.transforms.text_transforms import ParagraphSwap, BPEmbRandomVectorSwap, BPEmbVectorHide\n",
    "from ccp.encoders.ccp_text_encoder import TextEncoder\n",
    "\n",
    "from ccp.device_decision import DEVICE\n",
    "\n",
    "\n",
    "OUTPUT_DIRECTORY = \"ccp_labels\"\n",
    "\n",
    "encoder = TextEncoder(dim_in=(FORCED_LENGTH, EMBEDDING_DIMENSIONALITY)).to(DEVICE)\n",
    "\n",
    "projection_head = nn.Sequential(\n",
    "    nn.Linear(encoder.output_dim, PROJECTION_HIDDEN_DIM),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(PROJECTION_HIDDEN_DIM, PROJECTION_OUTPUT_DIM),\n",
    ").to(DEVICE)\n",
    "\n",
    "ccp_labeler = ContrastiveCredibilityLabeller(\n",
    "    data_reader = ag_data,\n",
    "    output_dir = OUTPUT_DIRECTORY,\n",
    "    transforms = [\n",
    "        Identity(), \n",
    "        GaussianNoise(), \n",
    "        ParagraphSwap(), \n",
    "        BPEmbRandomVectorSwap(embed),\n",
    "        BPEmbVectorHide(embed)\n",
    "    ],\n",
    "    encoder_network_f_b = encoder,\n",
    "    projection_head_f_z = projection_head,\n",
    "    batch_size = 64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14daa39f",
   "metadata": {},
   "source": [
    "### Learn the q-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1480874",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ccp.softlabeler.ccp_labeler import EMALossExitCriteria\n",
    "\n",
    "# The parameterization here is designed to run quickly on a CPU while learning very little.\n",
    "# To learn for real, use the defaults rather than this parameterization.\n",
    "ccp_inner_loop_iteration_exit_criterion = EMALossExitCriteria(ema_weight = 0.01, max_units_since_overwrite = 10, max_total_units = 20)\n",
    "\n",
    "# Prewarm the network before training\n",
    "ccp_labeler.prewarm_network(exit_criteria=ccp_inner_loop_iteration_exit_criterion)\n",
    "\n",
    "# Train by hand\n",
    "NUM_ITERATIONS = 3\n",
    "args = {\"previous_metadata\": None}\n",
    "for total_iterations in range(NUM_ITERATIONS):\n",
    "    previous_metadata = args[\"previous_metadata\"]\n",
    "    _, args = ccp_labeler.execute_ccp_single_iteration(exit_criteria = ccp_inner_loop_iteration_exit_criterion,\n",
    "                                                       output_prefix=f\"iteration_{total_iterations}\",\n",
    "                                                       previous_metadata=previous_metadata,\n",
    "                                                       print_loss_every_k_batches=5)\n",
    "    \n",
    "# Alternatively, train with automatic stopping condition, overwriting q vectors each time (no tracking of q values).\n",
    "# ccp_overall_exit_criterion = EMALossExitCriteria(ema_weight = 0.01, max_units_since_overwrite = 10, max_total_units = 20) # reset to defaults to learn for real\n",
    "# ccp_labeler.ema_train(ccp_overall_exit_criterion, \n",
    "#                       ccp_inner_loop_iteration_exit_criterion,\n",
    "#                       print_loss_every_k_batches=5\n",
    "#                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2b0015",
   "metadata": {},
   "source": [
    "### Learn the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ca002",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ccp.classifier.ccp_classifier import ContrastiveCredibilityClassifier\n",
    "from ccp.param_init import init_weights_ccp\n",
    "\n",
    "CLASSIFICATION_HIDDEN_DIM = 64\n",
    "NUM_TARGETS = ag_data.n_distinct_labels\n",
    "\n",
    "\n",
    "encoder = ccp_labeler.prewarmed_encoder()\n",
    "\n",
    "classifier_projection_head = nn.Sequential(\n",
    "    nn.Linear(encoder.output_dim, CLASSIFICATION_HIDDEN_DIM),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(CLASSIFICATION_HIDDEN_DIM, NUM_TARGETS),\n",
    ")\n",
    "\n",
    "ccp_classifier = ContrastiveCredibilityClassifier(encoder_network_f_b=encoder, \n",
    "                                                  projection_head_f_g=classifier_projection_head, \n",
    "                                                  q_dataset=ccp_labeler.classification_dataset(),\n",
    "                                                  batch_size=5,\n",
    "                                                  network_init_func=init_weights_ccp)\n",
    "\n",
    "# The parameterization here is designed to run quickly on a CPU while learning very little.\n",
    "# To learn for real, remove the parameterization to use defaults.\n",
    "exit_criterion = EMALossExitCriteria(ema_weight = 0.01, max_units_since_overwrite = 10, max_total_units = 20)\n",
    "\n",
    "# Run the classifier\n",
    "ccp_classifier.ema_train(exit_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84012849",
   "metadata": {},
   "source": [
    "# Evaluating CCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3853a5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Q-vector evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fd23c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ccp.typing import TargetLabel\n",
    "\n",
    "def build_comparison(q_vals, ag_data_all) -> np.typing.NDArray:\n",
    "    def _get_predicted_class(q_vals, item_idx) -> TargetLabel:\n",
    "        if all(q_vals[item_idx] == 0):\n",
    "            return ag_data_all.UNLABELLED_TARGET\n",
    "        else:\n",
    "            return q_vals[item_idx].argmax().item()\n",
    "\n",
    "    true_predicted = np.zeros((len(ag_data_all), 2))\n",
    "    for item_idx in range(len(ag_data_all)):\n",
    "        true_class = ag_data_all[item_idx][1]\n",
    "        predicted_class = _get_predicted_class(q_vals, item_idx)\n",
    "\n",
    "        true_predicted[item_idx, :] = (true_class, predicted_class)\n",
    "    return true_predicted\n",
    "\n",
    "def evaluate(q_vals, ag_data_all):\n",
    "    true_predicted = build_comparison(q_vals, ag_data_all)\n",
    "\n",
    "    for true_label in range(ag_data_all.n_distinct_labels):\n",
    "        data_subset = true_predicted[true_predicted[:, 0] == true_label]\n",
    "        num_correct = sum(data_subset[:, 1] == true_label)\n",
    "        num_unlabeled = sum(data_subset[:, 1] == ag_data_all.UNLABELLED_TARGET)\n",
    "        num_wrong = len(data_subset) - num_correct - num_unlabeled\n",
    "        print(f\"for class {true_label}, we have {num_correct / len(data_subset):0.2%} correct labels, {num_unlabeled / len(data_subset):0.2%} still unlabeled, and {num_wrong / len(data_subset):0.2%} wrong\")\n",
    "    \n",
    "\n",
    "# Same dataset as we used in training, but with NO data forgotten, so that we can compare the two labelsets\n",
    "ag_data_train_untransformed = AgNewsDataReader(split=\"train\", \n",
    "                                               rate_to_keep=1,\n",
    "                                               transform=embed,\n",
    "                                               sample_indices_to_use = SAMPLE_INDICES)\n",
    "\n",
    "for i in range(NUM_ITERATIONS):\n",
    "    print(f\"\\n\\n-------- Iteration {i} --------\")\n",
    "    q_vals = torch.load(f\"{OUTPUT_DIRECTORY}/iteration_{i}_q.pt\")\n",
    "    evaluate(q_vals, ag_data_train_untransformed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0ad922",
   "metadata": {},
   "source": [
    "## Classifier evaluation & comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from ccp.datareaders import DataReader\n",
    "\n",
    "def evaluate_classifier(classifier: nn.Module, data_reader: DataReader, batch_size: int = 100) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Assesses accuracy, which aligns with the CCP paper.\n",
    "    \n",
    "    Output is a tuple of (correct samples, total samples).\n",
    "    \"\"\"\n",
    "    correct_prediction_count = 0\n",
    "    for batched_data, true_labels in DataLoader(data_reader, batch_size = batch_size):\n",
    "        predictions = classifier(batched_data).argmax(dim=1)\n",
    "        correct_prediction_count += (true_labels == predictions).sum().item()\n",
    "\n",
    "    return correct_prediction_count, len(data_reader)\n",
    "\n",
    "###########\n",
    "# Train -- same as the data we learned on, but with labels\n",
    "ag_data_train_embedded = AgNewsDataReader(split=\"train\", \n",
    "                                          rate_to_keep=1,\n",
    "                                          transform=embed,\n",
    "                                          sample_indices_to_use = SAMPLE_INDICES)\n",
    "\n",
    "###########\n",
    "# Test\n",
    "ag_data_test_embedded = AgNewsDataReader(split=\"test\",\n",
    "                                         rate_to_keep=1,\n",
    "                                         transform=embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bfd5e4",
   "metadata": {},
   "source": [
    "### Performance of simple model with full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We will train on the same data, but without any \"forgetting\" of labels\n",
    "ag_full = AgNewsDataReader(split=\"train\", \n",
    "                           rate_to_keep=1,\n",
    "                           transform=embed,\n",
    "                           sample_indices_to_use = SAMPLE_INDICES)\n",
    "\n",
    "# This is NOT the CCP model -- we encode then predict\n",
    "nonccp_loader = DataLoader(ag_full, batch_size=256)\n",
    "nonccp_encoder = TextEncoder(dim_in=(FORCED_LENGTH, EMBEDDING_DIMENSIONALITY))\n",
    "nonccp_model_full = nn.Sequential(\n",
    "    nonccp_encoder,\n",
    "    nn.Linear(in_features=nonccp_encoder.output_dim, out_features=ag_full.n_distinct_labels)\n",
    ")\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonccp_model_full.parameters(),\n",
    "                            lr=0.1,\n",
    "                            weight_decay=1e-2,\n",
    "                            momentum=0.9)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"epoch {epoch} batches:\", end=\" \")\n",
    "    for which_batch_within_data, (X_batch, y_batch) in enumerate(nonccp_loader):\n",
    "        print(which_batch_within_data, end=\" \")\n",
    "        y_batch_pred = nonccp_model_full(X_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    print(f\"final loss in epoch {epoch} = {loss.item()}\")\n",
    "\n",
    "print()\n",
    "train_correct, train_total = evaluate_classifier(nonccp_model_full, ag_data_train_embedded)\n",
    "print(f\"Simple model with full data -- train accuracy: {train_correct / train_total:0.2%} ({train_correct}/{train_total})\")\n",
    "\n",
    "test_correct, test_total = evaluate_classifier(nonccp_model_full, ag_data_test_embedded)\n",
    "print(f\"Simple model with full data -- test accuracy: {test_correct / test_total:0.2%} ({test_correct}/{test_total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86666251",
   "metadata": {},
   "source": [
    "### Performance of simple model with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d480dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We reuse literally the same dataset from CCP, dropping -1 examples\n",
    "# But this is NOT the CCP model.\n",
    "nonccp_loader = DataLoader(ag_data, batch_size=256)\n",
    "nonccp_encoder = TextEncoder(dim_in=(FORCED_LENGTH, EMBEDDING_DIMENSIONALITY))\n",
    "nonccp_model_partial = nn.Sequential(\n",
    "    nonccp_encoder,\n",
    "    nn.Linear(in_features=nonccp_encoder.output_dim, out_features=ag_full.n_distinct_labels)\n",
    ")\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(nonccp_model_partial.parameters(),\n",
    "                            lr=0.1,\n",
    "                            weight_decay=1e-2,\n",
    "                            momentum=0.9)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"epoch {epoch} batches:\", end=\" \")\n",
    "    for which_batch_within_data, (X_batch, y_batch) in enumerate(nonccp_loader):\n",
    "        # drop the unlabeled data during training\n",
    "        X_batch = X_batch[y_batch != DataReader.UNLABELLED_TARGET]\n",
    "        y_batch = y_batch[y_batch != DataReader.UNLABELLED_TARGET]\n",
    "\n",
    "        y_batch_pred = nonccp_model_partial(X_batch)\n",
    "        loss = loss_fn(y_batch_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print()\n",
    "    print(f\"final loss in epoch {epoch} = {loss.item()}\")\n",
    "\n",
    "print()\n",
    "train_correct, train_total = evaluate_classifier(nonccp_model_partial, ag_data_train_embedded)\n",
    "print(f\"Simple model with {MISSING_DATA_RATE:0.0%} missing -- train accuracy: {train_correct / train_total:0.2%} ({train_correct}/{train_total})\")\n",
    "\n",
    "test_correct, test_total = evaluate_classifier(nonccp_model_partial, ag_data_test_embedded)\n",
    "print(f\"Simple model with {MISSING_DATA_RATE:0.0%} missing -- test accuracy: {test_correct / test_total:0.2%} ({test_correct}/{test_total})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f95fd",
   "metadata": {},
   "source": [
    "### Performance of CCP on missing data: classification after label recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946370ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_correct, train_total = evaluate_classifier(ccp_classifier.classifier(), ag_data_train_embedded)\n",
    "print(f\"CCP with {MISSING_DATA_RATE:0.0%} missing -- train accuracy: {train_correct / train_total:0.2%} ({train_correct}/{train_total})\")\n",
    "\n",
    "test_correct, test_total = evaluate_classifier(ccp_classifier.classifier(), ag_data_test_embedded)\n",
    "print(f\"CCP with {MISSING_DATA_RATE:0.0%} missing -- test accuracy: {test_correct / test_total:0.2%} ({test_correct}/{test_total})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}